day01---Hbase部分
1、Hbase的概念
(1).Hbase 是hadoop数据库，一个分布式的，可扩展的大数据存储。
(2).Hbase大量数据进行随机近实时读写操作数据库。
(3).Hbase也是一个开源的、分布式的、版本化的非关系型数据库。
(4).HBASE是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统。
2.Hbase也是常见的Nosql之一
非关系型数据库――列存储（HBase）
非关系型数据库――文档型存储（MongoDb）
非关系型数据库――内存式存储（redis）  KV
非关系型数据库――图形模型（Graph）

3.NoSQL(NoSQL = Not Only SQL )，意即“不仅仅是SQL”，具体来说就是跟关系型数据库有些类似（查询低延迟），
但同时能够存储的数据类型却更加灵活。

4.与传统数据库的对比
1、传统数据库遇到的问题：
1）数据量很大的时候无法存储
2）没有很好的备份机制
3）数据达到一定数量开始缓慢，很大的话基本无法支撑
 2、HBASE优势：
1）线性扩展，随着数据量增多可以通过节点扩展进行支撑
2）数据存储在hdfs上，备份机制健全
3）通过zookeeper协调查找数据，访问速度块。



二、Hbase的安装
集群规划：
Node Name	Master	 ZooKeeper	RegionServer
Hadoop001    yes     yes        yes
Hadoop002    backup  yes        yes
Hadoop003    no      yes        yes

(1).安装zk集群
vi zoo.cfg
tickTime=2000
initLimit=10
syncLimit=5
dataDir=/opt/zookeeper-3.4.7/zkmyid
clientPort=2181
server.1=Hadoop001:2888:3888
server.2=Hadoop002:2888:3888
server.3=Hadoop003:2888:3888

创建myid文件的目录：
mkdir /opt/zookeeper-3.4.7/zkmyid

在zkmyid目录下创建myid文件，并且写上对应的服务器标号

发送到Hadoop002、Hadoop003
scp -r zookeeper-3.4.7 hadoop@Hadoop002:/opt/
scp -r zookeeper-3.4.7 hadoop@Hadoop003:/opt/

分别修改Hadoop002、Hadoop003机器上的myid文件

配置环境变量
vi /etc/profile


(2).hbase环境搭建
hbase-env.sh
export JAVA_HOME= /opt/jdk1.8.0
export HBASE_MANAGES_ZK=false

hbase-site.xml
<property>
<!--hbase共享目录，持久化hbase数据-->
<name>hbase.rootdir</name>
<value>hdfs://Hadoop001:8020/hbase</value>
</property>
<!--#是否分布式运行，false即为单机-->
<property>
<name>hbase.cluster.distributed</name>  
<value>true</value>
</property>
<property>
<name>hbase.zookeeper.quorum</name>#zookeeper地址
<value>Hadoop001,Hadoop002,Hadoop003</value>
</property>
<property>
<name>hbase.zookeeper.property.dataDir</name>#zookeeper配置信息快照的位置
<value>/opt/hbase-1.1.2/tmp/zookeeper</value>
</property>

regionservers
Hadoop001
Hadoop002
Hadoop003

在conf目录下创建一个备用master：
backup-masters
Hadoop002


发送到Hadoop002、Hadoop003
scp -r /opt/hbase-1.1.2 hadoop@Hadoop002:/opt/
scp -r /opt/hbase-1.1.2 hadoop@Hadoop003:/opt/

启动Hbase
注意：需要时间同步：
启动zookeepr
启动hdfs
在启动hbase



查看进程 ：jps


查看webUI
http://192.168.57.11:16010

进入Hbase的客户端：
hbase shell
进入之后 list命令查看hbase连接zk成功


三、hbase的 shell
1.命名空间
创建命名空间：help 'namespace'
create_namespace 'ns1'

查看命名空间的描述
describe_namespace 'ns1'

查看所有的命名空间
list_namespace

显示命名空间下所有表：
list_namespace_tables 'hbase'

删除命名空间：
drop_namespace 'ns1'

2. 创建表 help 'create'
在指定的命名空间下创建表
create 'ns1:stu','cf_info','cf_other'
在默认的命名空间下创建表：
create 'stu01','cf_info','cf_other'

3.添加数据：help 'put'
put 'ns1:stu3','rk001','cf_info:name','xiao'
put 'ns1:stu3','rk001','cf_info:age',10
put 'ns1:stu3','rk001','cf_info:sex','nan'
put 'ns1:stu3','rk001','cf_other:address','qingmang'
            
put 'ns1:stu3','rk002','cf_info:name','li'
put 'ns1:stu3','rk002','cf_info:age',14
put 'ns1:stu3','rk002','cf_info:sex','nv'
put 'ns1:stu3','rk002','cf_info:gid','AAA'
put 'ns1:stu3','rk002','cf_other:address','beicai'
            
put 'ns1:stu3','rk003','cf_info:name','zhang'
put 'ns1:stu3','rk003','cf_info:age',15
put 'ns1:stu3','rk003','cf_info:sex','nan'
put 'ns1:stu3','rk003','cf_other:address','beijing'

4.查看数据：help 'scan'
查看所有列族：
scan 'ns1:stu'

查看某一个列族：
scan 'ns1:stu'
scan 'ns1:stu', {COLUMNS => 'cf_info'}
scan 'ns1:stu', {COLUMNS => 'cf_info:name'}

5.查看数据：help 'get'
查看某一行数据：
get 'ns1:stu', 'rk003'
查看某一行数据的某一个列族：
get 'ns1:stu', 'rk003',{COLUMN => 'cf_info'}
get 'ns1:stu', 'rk003',{COLUMN => 'cf_info:name'}
get 'ns1:stu', 'rk003','cf_info:name'

6.删除数据；使用另案例：
create 'test', {NAME=>'e', VERSIONS=>2147483647, KEEP_DELETED_CELLS => true}
put 'test', 'r1', 'e:c1', 10
put 'test', 'r1', 'e:c1', 11
put 'test', 'r1', 'e:c1', 12
put 'test', 'r1', 'e:c1', 13
put 'test', 'r1', 'e:c1', 14

删除数据：
delete 'test', 'r1', 'e:c1', 1500276727419

7.删除表： help 'drop'
删除表之前判断表是否禁用：
如果没有禁用，先禁用，在删除
disable 'stu01' 或者 enable 'stu01'
drop 'stu01'


count 'ns1:stu'

删除整行：
deleteall 'ns1:stu','rk002'

四、Hbse的API

五、Hbase的过滤器


六、Hive和Hbase整合理论
1、为什么hive要和hbase整合
2、整合的优缺点
优点：
(1).Hive方便地提供了Hive QL的接口来简化MapReduce的使用，
  而HBase提供了低延迟的数据库访问。如果两者结合，可以利
  用MapReduce的优势针对HBase存储的大量内容进行离线的计算和分析。
(2).操作方便，hive提供了大量系统功能
缺点：
  性能的损失，hive有这样的功能, 他支持通过类似sql语句的语法来操作hbase
  中的数据, 但是速度慢。

3、整合需要做什么样的准备工作
4、整合后的目标
(1). 在hive中创建的表能直接创建保存到hbase中。
(2). 往hive中的表插入数据，数据会同步更新到hbase对应的表中。
(3). hbase对应的列簇值变更，也会在Hive中对应的表中变更。
(4). 实现了多列，多列簇的转化：（示例：hive中3列对应hbase中2列簇）

5、hive和Hbase整合后如果通信？
查看hive和Hbase通信图：
主要是通过hive 的lib目录下的hive-hbase-handler-1.2.1.jar来实现hive和Hbase通信。

二、Hive和Hbase整合案例操作
创建Hive普通表：
create table if not exists tb_emp(
emp_id int,
emp_name String,
emp_age int,
emp_address String
)
row format delimited fields terminated by ',';

vi tb_emp.txt
10010,xiaoliu,18,qingmang
10011,xiaomei,16,qingmang
10012,xiaoming,16,beijjng
10013,xiaoli,18,qingmang

load data local inpath '/opt/data/tb_emp.txt' into table  tb_emp;

(1).创建一个HIve和HBase识别表：
create table if not exists hh01_emp(
emp_id int,
emp_name String,
emp_age int,
emp_address String
)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf_emp:emp_name,cf_emp:emp_age,cf_other:emp_address")   
TBLPROPERTIES ("hbase.table.name" = "hh01");

说明：
1.STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
使用Hive的lib目录下的hive-hbase-handler-1.2.0.jar类实现对Hive和Hbase识别表管理
2.WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf_emp:emp_name,cf_emp:emp_age,cf_other:emp_address")  
hive会把第一列给予":key",作为Hbase的rowkey
3.TBLPROPERTIES ("hbase.table.name" = "hh01");
表示在Hbase中识别表的名称为hh01，如果是在指定的命名空间下>xx:hh01


导入数据到Hive和HBase识别表
不能用load data方式添加导数据
load data local inpath '/opt/data/tb_emp.txt' into table  hh01_emp;
使用结果集的方式导入数据：
insert into table hh01_emp select * from tb_emp;

查看hh01_emp的库下面有没有数据
hdfs://Hadoop001:8020/user/hive/warehouse/emp_db.db/hh01_emp;

查看Hbase的表有没有数据
 scan 'hh01'

 在hbase中添加数据
 put 'hh01','10014','cf_emp:emp_name','xiaowang'
 put 'hh01','10014','cf_emp:emp_age','21'
 put 'hh01','10014','cf_other:emp_address','tianijing'
 
 再在Hbase查看：
 scan 'hh01'
 
 在hive里面查看：
 select * from hh01_emp;
 
 (2).如何HBase已经存在一个Hbase数据表：比如"ns2:stu"
 注意：此时Hbase已经存在stu表
 问题：如何在Hive中创建一个Hive表来识别已经存在的Hbase表。
 
在hive中创建hive识别表：
create EXTERNAL table if not exists hh02_emp(
nid String,
name String,
age int,
sex String,
address String
)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf_info:name,cf_info:age,cf_info:sex,cf_other:address")   
TBLPROPERTIES ("hbase.table.name" = "ns2:stu");

说明：
1.因为Hbase已经存在Hbase表，所以Hive识别表必须是EXTERNAL类型




Hive与Hbase和mapreduce  
利用两者本身对外的API来实现整合，
主要是靠HBaseStorageHandler进行通信，利用 HBaseStorageHandler，
Hive可以获取到Hive表对应的HBase表名，列簇以及列，InputFormat和
OutputFormat类，创建和删除HBase表等。
    Hive访问HBase中表数据，实质上是通过MapReduce读取HBase表数据，
其实现是在MR中，使用HiveHBaseTableInputFormat完成对HBase表的切分，
获取RecordReader对象来读取数据。对HBase表的切分原则是一个Region切
分成一个Split,即表中有多少个Regions,MR中就有多少个Map；读取HBase表
数据都是通过构建Scanner，对表进行全表扫描，如果有过滤条件，则转化
为Filter。当过滤条件为rowkey时，则转化为对rowkey的过滤；Scanner通过
RPC调用RegionServer的next()来获取数据.



（3）、性能瓶颈分析
1. Map Task
Hive读取HBase表，通过MR,最终使用HiveHBaseTableInputFormat来读取数据，在getSplit()方法中对 HBase表进行切分，切分原则是根据该表对应的HRegion，将每一个Region作为一个InputSplit，即，该表有多少个Region,就 有多少个Map Task；
每个Region的大小由参数hbase.hregion.max.filesize控制，默认10G，这样会使得每个map task处理的数据文件太大，map task性能自然很差；
为HBase表预分配Region，使得每个Region的大小在合理的范围；
2. Scan RPC 调用：
在Scan中的每一次next()方法都会为每一行数据生成一个单独的RPC请求， 
全表有2500万行记录，因此要2500万次RPC请求；
  扫描器缓存（Scanner Caching）：HBase为扫描器提供了缓存的功能，
可以通过参数hbase.client.scanner.caching来设置；默认是100；
缓存 的原理是通过设置一个缓存的行数，当客户端通过RPC请求RegionServer获取数据时，
RegionServer先将数据缓存到内存，当缓存的数 据行数达到参数设置的数量时，
再一起返回给客户端。这样，通过设置扫描器缓存，就可以大幅度减少客户端RPC调用
RegionServer的次数；但并不 是缓存设置的越大越好，如果设置的太大，每一次RPC调用
将会占用更长的时间，因为要获取更多的数据并传输到客户端，如果返回给客户端的数据
超出了其堆的 大小，程序就会终止并跑出OOM异常；



七、flume和Hbase的使用
===========>先创建Hbase表和列族<================
================================================================================
#说明：案例是flume监听目录/opt/test采集到hbase；必须先在Hbase中创建表和列族

数据目录：
vi /opt/test/word.txt
1001 panzong nan
2200 lili nv

vi flume-hbase.conf
#Name the  components on this agent  
a1.sources  = r1  
a1.sinks =  k1  
a1.channels  = c1  
  
#Describe/configure the source  
a1.sources.r1.type  = spooldir  
a1.sources.r1.spoolDir=/opt/test 
  
# Describe  the sink  
a1.sinks.k1.type =asynchbase
a1.sinks.k1.table = tb_words
a1.sinks.k1.columnFamily = words
#目前自己处理到支持一个列名的，多个列名称失败了，多个列名考虑使用下面的案例的正则表达式方式匹配 
a1.sinks.k1.serializer.payloadColumn=wd  
a1.sinks.k1.serializer =org.apache.flume.sink.hbase.SimpleAsyncHbaseEventSerializer
# Use a  channel which buffers events in memory  
a1.channels.c1.type  = memory  
a1.channels.c1.capacity  = 1000  
a1.channels.c1.transactionCapacity  = 100  
  
# Bind the  source and sink to the channel  
a1.sources.r1.channels  = c1  
a1.sinks.k1.channel  = c1


执行命令：
./flume-ng agent -c . -f /opt/flume-1.7.0/conf/flume-hbase.conf -n a1 -Dflume.root.logger=INFO,console

案例2：使用正则表达式，对行分多个列值
说明：apache-flume-1.7.0-bin.tar.gz 和 Hbase-1.12+
必须在在Hbase中创建表
================================================================================
数据目录：
vi /opt/test/data.txt
1001,panzong,nan
2200,lili,nv


flume配置文件：
vi flume-2-hbase.conf
#Name the  components on this agent
a1.sources  = r1
a1.sinks =  k1
a1.channels  = c1

#Describe/configure the source
a1.sources.r1.type  = spooldir
a1.sources.r1.spoolDir=/opt/test

# Describe  the sink
#a1.sinks.k1.type =org.apache.flume.sink.hbase.HBaseSink
a1.sinks.k1.type =hbase
a1.sinks.k1.table = students
a1.sinks.k1.columnFamily = info
a1.sinks.k1.serializer = org.apache.flume.sink.hbase.RegexHbaseEventSerializer
a1.sinks.k1.serializer.regex= ^([0-9]+),([a-z]+),([a-z]+)$
# 指定某一列来当主键，而不是用随机生成的key,#第一列为Hbase的rowkey
a1.sinks.k1.serializer.rowKeyIndex = 0
#ROW_KEY为系统指定列名
a1.sinks.k1.serializer.colNames= ROW_KEY,name,sex
a1.sinks.k1.zookeeperQuorum =Hadoop001:2181,Hadoop002:2181,Hadoop003:2181

# Use a  channel which buffers events in memory
a1.channels.c1.type  = memory
a1.channels.c1.capacity  = 1000
a1.channels.c1.transactionCapacity  = 100

# Bind the  source and sink to the channel
a1.sources.r1.channels  = c1
a1.sinks.k1.channel  = c1


执行命令：
./flume-ng agent -c . -f /opt/flume-1.7.0/conf/flume-2-hbase.conf -n a1 -Dflume.root.logger=INFO,console


#第二列为Hbase的rowkey
#a1.sinks.k1.serializer.rowKeyIndex = 1
#a1.sinks.k1.serializer.regex= ^([0-9]+),([a-z]+),([a-z]+)$
#a1.sinks.k1.serializer.colNames= name,ROW_KEY,sex


八、Hbase和MapReuce的整合
1.需求：读取Hbase数据，使用MapReduce清洗完成之后保存到HDFS系统



kafka部分
一、概念
1.官网：
http://kafka.apache.org/downloads
2.Kafka是一种高吞吐量的分布式发布--订阅消息系统。
它可以处理消费者规模的网站中的所有动作流数据。

3.Kafka的目的是通过Hadoop的并行加载机制来统一线上和离线的消息处理，
也是为了通过集群机来提供实时的消费。

4.Kafka是一个分布式的流处理平台。
5.kafka的特性：
kafka主要是作为一个分布式的、可分区的、具有副本数的日志服务系统, 
具有高水平扩展性、高容错性、访问速度快、分布式等特性

6.主要应用场景是：日志收集系统和分布式发布--订阅消息中间插件

7.kafka_2.11-0.9.0.1
说明是由Scala语言编写的，当前kafka框架的scala是2.11,kafka版本是0.9.x


二、kafka环境的搭建
1.需要安装JDK1.8以上
2.安装zk集群
3.安装kafka
核心配置文件：
consumer.properties #配置消费者属性
producer.properties # 生产者属性
server.properties   #服务broker消息
zookeeper.properties #zk属性

配置：
server.properties   #服务broker消息
broker.id=0 （kafka节点ID，必须是唯一）
log.dirs=/opt/kafka_2.11_0.9/logdir  （kafka日志存放目录）
log.retention.hours=168 （消息保存最长的时间）
log.segment.bytes=1073741824 （每一个segment文件的大小，默认是1G，可以更改）
zookeeper.connect=Hadoop001:2181,Hadoop002:2181,Hadoop003:2181 （kafka 连接zk的信息，必须配置）
delete.topic.enable=true （kafka 的topic是否物理删除，true-物理删除，false-逻辑删除）
host.name=Hadoop001 （各个kafka节点的主机名）

配置完成，将kafka框架发送到其他节点
 scp -r kafka_2.11_0.9 hadoop@Hadoop002:/opt/
 scp -r kafka_2.11_0.9 hadoop@Hadoop003:/opt/

 发送完成之后，修改每个节点上的broker.id为唯一值！！host.name为当前节点的主机名！！！
 
 
 修改环境变量：
 export KAFKA_HOME=/opt/kafka_2.11_0.9
 :$KAFKA_HOME/bin
 
 发送环境变量配置文件到其他节点：
 sudo scp /etc/profile root@Hadoop002:/etc/
 sudo scp /etc/profile root@Hadoop003:/etc/
 
 
 启动Kafka环境：
 1.启动zk集群
 2.启动Kafka：
 方式一： 前台线程启动方式启动kafka
 kafka-server-start.sh /opt/kafka_2.11_0.9/config/server.properties
 方式二：后台守护进程方式启动：
 分别在三台服务器上执行
  nohup kafka-server-start.sh /opt/kafka_2.11_0.9/config/server.properties &
  
  到三台服务器上jps查看：是否有Kafka
  
  关闭kafka
  nohup kafka-server-stop.sh /opt/kafka_2.11_0.9/config/server.properties &
  
 
 三、Kafka常用操作命令
 1.查看当前服务器中的所有topic
  kafka-topics.sh --list --zookeeper  Hadoop001:2181
  想想为什么后面要链接zk？？？？
  
  2. 创建topic
  要求：
  topic的名称为：OrderTopic
  topic的分区数量：2
  topic的副本数：2
  kafka-topics.sh --create --zookeeper Hadoop001:2181 --replication-factor 2 --partitions 2 --topic OrderTopic

  3.删除topic
  需要server.properties中设置delete.topic.enable=true否则只是标记删除或者直接重启。

     kafka-topics.sh --delete --zookeeper Hadoop001:2181 --topic OrderTopic
  4.通过shell命令发送消息(生产者)
    kafka-console-producer.sh --broker-list Hadoop001:9092 --topic OrderTopic
	
	在屏幕输入内容，然后回车就表示生产数据到TOpic的分区里面，
	然后可以去分区里面查看...log大小是否增加！！！
   
   5.通过shell消费消息（消费者）
   kafka-console-consumer.sh --zookeeper Hadoop001:2181 --from-beginning --topic OrderTopic
   
   6.查看消费位置
   说明：0.9以前可以使用下面方式查看消费者：
   kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper Hadoop001:2181 --group testGroup
   0.9.x 后使用消费者位置：
   kafka-run-class.sh kafka.admin.ConsumerGroupCommand --zookeeper Hadoop001:2181 --describe --group test-consumer-group
   
   7.查看某个Topic的详情
    kafka-topics.sh --topic OrderTopic --describe --zookeeper Hadoop001:2181

  8.新增配置
  kafka-topics.sh --zookeeper Hadoop001:2181 --create --topic My-Topic --partitions 3 --replication-factor 2 --config max.message.bytes=64000 --config flush.messages=1 
  
  9.修改Topic的配置信息
  kafka-topics.sh --zookeeper Hadoop001:2181 --alter --topic My-Topic --config max.message.bytes=128000
  
  10.删除配置：
  kafka-topics.sh --zookeeper Hadoop001:2181 --alter --topic My-Topic --delete-config max.message.bytes


   四、Kafka APi的使用
   (1).使用maven方式开发
   IDEA安装maven插件->window环境下安装apache-maven-3.0.5-->IDEA 配置maven
   
   
	
	
	


  


